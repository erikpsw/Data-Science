{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-16T08:38:30.593825Z","iopub.status.busy":"2024-10-16T08:38:30.593120Z","iopub.status.idle":"2024-10-16T08:38:38.143246Z","shell.execute_reply":"2024-10-16T08:38:38.142242Z","shell.execute_reply.started":"2024-10-16T08:38:30.593783Z"},"papermill":{"duration":1.594977,"end_time":"2024-10-10T11:58:33.569648","exception":false,"start_time":"2024-10-10T11:58:31.974671","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import joblib \n","\n","import pandas as pd\n","import polars as pl\n","import lightgbm as lgb\n","import xgboost as xgb\n","import catboost as cbt\n","import numpy as np \n","\n","from joblib import Parallel, delayed\n","\n","\n","# !pip install lightgbm==4.2.0 -i https://mirrors.aliyun.com/pypi/simple/\n","# !pip install catboost==1.2.7 -i https://mirrors.aliyun.com/pypi/simple/\n","# !pip install xgboost==2.0.3 -i https://mirrors.aliyun.com/pypi/simple/\n","# !pip install joblib==1.4.2 -i https://mirrors.aliyun.com/pypi/simple/\n","\n","\n","def reduce_mem_usage(df, float16_as32=True):\n","    #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    for col in df.columns:#遍历每列的列名\n","        col_type = df[col].dtype#列名的type\n","        if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n","            c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n","            if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n","                #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:#如果是浮点数类型.\n","                #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    if float16_as32:#如果数据需要更高的精度可以选择float32\n","                        df[col] = df[col].astype(np.float32)\n","                    else:\n","                        df[col] = df[col].astype(np.float16)  \n","                #如果数值在float32的取值范围内，对它进行类型转换\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                #如果数值在float64的取值范围内，对它进行类型转换\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","    #计算一下结束后的内存\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    #相比一开始的内存减少了百分之多少\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","\n","    return df"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T08:38:41.681883Z","iopub.status.busy":"2024-10-16T08:38:41.680760Z"},"trusted":true},"outputs":[{"ename":"ArrowMemoryError","evalue":"realloc of size 1091264 failed","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)","Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# If in training mode, load the training data\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAINING:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Load the training data from a Parquet file\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Reduce memory usage of the DataFrame (function not provided here)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     df \u001b[38;5;241m=\u001b[39m reduce_mem_usage(df, \u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   1833\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   1834\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1841\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1478\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1479\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   1480\u001b[0m         ]\n\u001b[0;32m   1481\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   1483\u001b[0m         )\n\u001b[1;32m-> 1485\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pyarrow\\_dataset.pyx:562\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pyarrow\\_dataset.pyx:3804\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n","File \u001b[1;32me:\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n","\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 1091264 failed"]}],"source":["# Define the path to the input data directory  \n","# If the local directory exists, use it; otherwise, use the Kaggle input directory\n","input_path = 'jane-street-real-time-market-data-forecasting/'\n","\n","# Flag to determine if the script is in training mode or not\n","TRAINING = True\n","\n","# Define the feature names based on the number of features (79 in this case)\n","feature_names = [f\"feature_{i:2d}\" for i in range(79)]\n","\n","# Number of validation dates to use\n","num_valid_dates = 100\n","\n","# Number of dates to skip from the beginning of the dataset\n","skip_dates = 500\n","\n","# Number of folds for cross-validation\n","N_fold = 5\n","\n","# If in training mode, load the training data\n","if TRAINING:\n","    # Load the training data from a Parquet file\n","    df = pd.read_parquet(f'{input_path}/train.parquet')\n","    \n","    # Reduce memory usage of the DataFrame (function not provided here)\n","    df = reduce_mem_usage(df, False)\n","    \n","    # Filter the DataFrame to include only dates greater than or equal to skip_dates\n","    df = df[df['date_id'] >= skip_dates].reset_index(drop=True)\n","    \n","    # Get unique dates from the DataFrame\n","    dates = df['date_id'].unique()\n","    \n","    # Define validation dates as the last `num_valid_dates` dates\n","    valid_dates = dates[-num_valid_dates:]\n","    \n","    # Define training dates as all dates except the last `num_valid_dates` dates\n","    train_dates = dates[:-num_valid_dates]\n","    \n","    # Display the last few rows of the DataFrame (for debugging purposes)\n","    print(df.tail())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create a directory to store the trained models\n","os.system('mkdir models')\n","\n","# Define the path to load pre-trained models (if not in training mode)\n","model_path = '/kaggle/input/jsbaselinezyz'\n","\n","# If in training mode, prepare validation data\n","if TRAINING:\n","    # Extract features, target, and weights for validation dates\n","    X_valid = df[feature_names].loc[df['date_id'].isin(valid_dates)]\n","    y_valid = df['responder_6'].loc[df['date_id'].isin(valid_dates)]\n","    w_valid = df['weight'].loc[df['date_id'].isin(valid_dates)]\n","\n","# Initialize a list to store trained models\n","models = []\n","\n","# Function to train a model or load a pre-trained model\n","def train(model_dict, model_name='lgb'):\n","    if TRAINING:\n","        # Select dates for training based on the fold number\n","        selected_dates = [date for ii, date in enumerate(train_dates) if ii % N_fold != i]\n","        \n","        # Get the model from the dictionary\n","        model = model_dict[model_name]\n","        \n","        # Extract features, target, and weights for the selected training dates\n","        X_train = df[feature_names].loc[df['date_id'].isin(selected_dates)]\n","        y_train = df['responder_6'].loc[df['date_id'].isin(selected_dates)]\n","        w_train = df['weight'].loc[df['date_id'].isin(selected_dates)]\n","\n","        # Train the model based on the type (LightGBM, XGBoost, or CatBoost)\n","        if model_name == 'lgb':\n","            # Train LightGBM model with early stopping and evaluation logging\n","            model.fit(X_train, y_train, w_train,  \n","                      eval_metric=[r2_lgb],\n","                      eval_set=[(X_valid, y_valid, w_valid)], \n","                      callbacks=[\n","                          lgb.early_stopping(100), \n","                          lgb.log_evaluation(10)\n","                      ])\n","            \n","        elif model_name == 'cbt':\n","            # Prepare evaluation set for CatBoost\n","            evalset = cbt.Pool(X_valid, y_valid, weight=w_valid)\n","            \n","            # Train CatBoost model with early stopping and verbose logging\n","            model.fit(X_train, y_train, sample_weight=w_train, \n","                      eval_set=[evalset], \n","                      verbose=10, \n","                      early_stopping_rounds=100)\n","            \n","        else:\n","            # Train XGBoost model with early stopping and verbose logging\n","            model.fit(X_train, y_train, sample_weight=w_train, \n","                      eval_set=[(X_valid, y_valid)], \n","                      sample_weight_eval_set=[w_valid], \n","                      verbose=10, \n","                      early_stopping_rounds=100)\n","\n","        # Append the trained model to the list\n","        models.append(model)\n","        \n","        # Save the trained model to a file\n","        joblib.dump(model, f'./models/{model_name}_{i}.model')\n","        \n","        # Delete training data to free up memory\n","        del X_train\n","        del y_train\n","        del w_train\n","        \n","        # Collect garbage to free up memory\n","        import gc\n","        gc.collect()\n","        \n","    else:\n","        # If not in training mode, load the pre-trained model from the specified path\n","        models.append(joblib.load(f'{model_path}/{model_name}_{i}.model'))\n","        \n","    return \n","\n","# Custom R2 metric for XGBoost\n","def r2_xgb(y_true, y_pred, sample_weight):\n","    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n","    return -r2\n","\n","# Custom R2 metric for LightGBM\n","def r2_lgb(y_true, y_pred, sample_weight):\n","    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n","    return 'r2', r2, True\n","\n","# Custom R2 metric for CatBoost\n","class r2_cbt(object):\n","    def get_final_error(self, error, weight):\n","        return 1 - error / (weight + 1e-38)\n","\n","    def is_max_optimal(self):\n","        return True\n","\n","    def evaluate(self, approxes, target, weight):\n","        assert len(approxes) == 1\n","        assert len(target) == len(approxes[0])\n","\n","        approx = approxes[0]\n","\n","        error_sum = 0.0\n","        weight_sum = 0.0\n","\n","        for i in range(len(approx)):\n","            w = 1.0 if weight is None else weight[i]\n","            weight_sum += w * (target[i] ** 2)\n","            error_sum += w * ((approx[i] - target[i]) ** 2)\n","\n","        return error_sum, weight_sum\n","\n","# Dictionary to store different models with their configurations\n","model_dict = {\n","    'lgb': lgb.LGBMRegressor(n_estimators=500, device='gpu', gpu_use_dp=True, objective='l2'),\n","    'xgb': xgb.XGBRegressor(n_estimators=2000, learning_rate=0.1, max_depth=6, tree_method='hist', device=\"cuda\", objective='reg:squarederror', eval_metric=r2_xgb, disable_default_eval_metric=True),\n","    'cbt': cbt.CatBoostRegressor(iterations=1000, learning_rate=0.05, task_type='GPU', loss_function='RMSE', eval_metric=r2_cbt()),\n","}\n","\n","# Train models for each fold\n","for i in range(N_fold):\n","    train(model_dict, 'lgb')\n","    train(model_dict, 'xgb')\n","    train(model_dict, 'cbt')"]},{"cell_type":"markdown","metadata":{},"source":["#### There seems to be bug in official code, can only submit polars dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.018344,"end_time":"2024-10-10T11:58:33.59684","exception":false,"start_time":"2024-10-10T11:58:33.578496","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["lags_ : pl.DataFrame | None = None\n","\n","# Replace this function with your inference code.\n","# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n","# Each batch of predictions (except the very first) must be returned within 10 minutes of the batch features being provided.\n","def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n","    \"\"\"Make a prediction.\"\"\"\n","    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n","    # Use them as extra features, if you like.\n","    global lags_\n","    if lags is not None:\n","        lags_ = lags\n","\n","    predictions = test.select(\n","        'row_id',\n","        pl.lit(0.0).alias('responder_6'),\n","    )\n","    \n","    feat = test[feature_names].to_numpy()\n","    \n","    pred = [model.predict(feat) for model in models]\n","    pred = np.mean(pred, axis=0)\n","    \n","    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n","\n","    # The predict function must return a DataFrame\n","    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n","    # with columns 'row_id', 'responer_6'\n","    assert list(predictions.columns) == ['row_id', 'responder_6']\n","    # and as many rows as the test data.\n","    assert len(predictions) == len(test)\n","\n","    return predictions"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.002521,"end_time":"2024-10-10T11:58:33.6023","exception":false,"start_time":"2024-10-10T11:58:33.599779","status":"completed"},"tags":[]},"source":["When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first `predict` call, which does not have the usual 10 minute response deadline."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.225871,"end_time":"2024-10-10T11:58:35.830964","exception":false,"start_time":"2024-10-10T11:58:33.605093","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n","\n","if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n","    inference_server.serve()\n","else:\n","    inference_server.run_local_gateway(\n","        (\n","            '/kaggle/input/jane-street-realtime-marketdata-forecasting/test.parquet',\n","            '/kaggle/input/jane-street-realtime-marketdata-forecasting/lags.parquet',\n","        )\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9849268,"sourceId":84493,"sourceType":"competition"},{"datasetId":5875295,"sourceId":9625192,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"papermill":{"default_parameters":{},"duration":7.594014,"end_time":"2024-10-10T11:58:36.355301","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-10T11:58:28.761287","version":"2.6.0"}},"nbformat":4,"nbformat_minor":4}
