{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distinctive_tokens(token_sets, max_occur):\n",
    "    # 1. Count the number of clusters each token appears in\n",
    "    token_cluster_count = {}\n",
    "    for tokens in token_sets.values():\n",
    "        for token in tokens:\n",
    "            token_cluster_count[token] = token_cluster_count.get(token, 0) + 1\n",
    "\n",
    "    # 2. Find distinctive tokens for each cluster\n",
    "    distinctive = {}\n",
    "    for cluster_id, tokens in token_sets.items():\n",
    "        # Select tokens that appear in max_occur or fewer clusters\n",
    "        distinctive_tokens = {token for token in tokens if token_cluster_count[token] <= max_occur}\n",
    "        # If a cluster has distinctive tokens, add them to the dictionary\n",
    "        if distinctive_tokens:\n",
    "            distinctive[cluster_id] = distinctive_tokens\n",
    "\n",
    "    return distinctive\n",
    "\n",
    "# Example usage:\n",
    "# Assume token_sets is a dictionary where keys are cluster IDs and values are sets of top tokens for each cluster.\n",
    "# Assume max_occur is the maximum number of clusters a token can appear in to be considered distinctive.\n",
    "# distinctive_tokens = find_distinctive_tokens(token_sets, max_occur)\n",
    "# print(distinctive_tokens)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_top_tokens(cid, labels, corpusdf, k):\n",
    "    # 1. Filter the documents for the given cluster ID\n",
    "    cluster_docs = corpusdf[labels == cid]\n",
    "    \n",
    "    # 2. Concatenate all pseudo-documents into one large string\n",
    "    all_tokens = ' '.join(cluster_docs['pseudodoc']).split()\n",
    "    \n",
    "    all_tokens=sorted(all_tokens)\n",
    "    # 3. Count each unique token's frequency\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # 4. Return the top k tokens, sorted by frequency and token in case of tie\n",
    "    top_tokens = set(token for token, _ in token_counts.most_common(k))\n",
    "    \n",
    "    # 5. In case there are fewer than k tokens, return as many as available\n",
    "    # Note: The set will naturally have fewer than k elements if not enough unique tokens are present.\n",
    "    \n",
    "    return top_tokens\n",
    "\n",
    "# Example usage:\n",
    "# Assuming the variables `labels` and `corpusdf` have been defined earlier and `cid` is the cluster ID you are interested in.\n",
    "# top_tokens = get_top_tokens(cid, labels, corpusdf, k)\n",
    "# print(top_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
